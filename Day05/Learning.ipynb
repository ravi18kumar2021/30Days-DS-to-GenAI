{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGP1sNj0DoCHz7V3tYscV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravi18kumar2021/30Days-DS-to-GenAI/blob/main/Day05/Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "tTacgR2syoUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification is a type of supervised learning, where the goal is to predict categorical labels. Each input is mapped to a discrete class."
      ],
      "metadata": {
        "id": "kwHkvhv_yoRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary vs Multi-Class Classification"
      ],
      "metadata": {
        "id": "XqdFXQtQyoOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary classification deals with exactly two possible output labels. Eg. spam or not spam mail, diabetic or non-diabetic patient, etc.\n",
        "\n",
        "Multi-class classification handles three or more mutually exclusive classes. Eg. Digit recognition (0-9), animal types: cat, dog, horse, etc."
      ],
      "metadata": {
        "id": "3yDw0qvoyoMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Intuition\n",
        "Unlike Linear Regression (which predicts continuous values), Logistic Regression predicts probability of outcomes between 0 and 1.\n",
        "\n",
        "It uses sigmoid function, to convert outputs into a probability.\n",
        "\n",
        "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "if $\\sigma(z) ≥ 0.5 ⇒ $ Class 1 and $\\sigma(z) < 0.5 ⇒ $ Class 0"
      ],
      "metadata": {
        "id": "AEktjH2iyoJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Boundary"
      ],
      "metadata": {
        "id": "QnSLxym4yoGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision boundary is the dividing line (or curve) that a classification model uses to separate different classes in the feature space. In higher dimensions, the decision boundary becomes a plane in 3D and a hyperplane in the above.\n",
        "\n",
        "Logistic Regression is limited to linear boundaries. In case of non-linear data, we use:\n",
        "- non-linear feature transformations like polynomial, RBF.\n",
        "- more flexible models like SVMs, decision trees or neural networks."
      ],
      "metadata": {
        "id": "IG80PPjDyoCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "gz_wJkpuyn_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making predictions, we compare them with true labels using a confusion matrix:\n",
        "\n",
        "| |Predicted Positive|Predicted Negative|\n",
        "|-|------------------|------------------|\n",
        "|Actual Positive|True Positive (TP)|False Negative (FN)|\n",
        "|Actual Negative|False Positive (FP)|True Negative (TN)|"
      ],
      "metadata": {
        "id": "gx6BfX1fyn9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Metrics"
      ],
      "metadata": {
        "id": "tR2Re8Amyn68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using confusion matrix, we compute:\n",
        "- Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "- Precision = $\\frac{TP}{TP + FP}$ , how many predicted positives were actually positive?\n",
        "- Recall = $\\frac{TP}{TP + FN}$ , how many actual positives did we catch?\n",
        "- F1 Score = $2*\\frac{Precision * Recall}{Precision + Recall}$, harmonic mean of precision and recall"
      ],
      "metadata": {
        "id": "8PNYJ5dt7VlM"
      }
    }
  ]
}